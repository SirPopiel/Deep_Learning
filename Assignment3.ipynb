{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "# Set the seed for easy reproducibility\n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)  \n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Set GPU memory growth \n",
    "# Allows to only as much GPU memory as needed\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this challenge, a custom generator had to be built. Therefore, I proceeded to build a train/validation generator, which yielded a batch composed of random questions sampled from the training set with the corresponding images and answers. Before, though, I tokenize the words contained in the training questions and, then, I create my embedding matrix, which i will then use in my first embedding layer of the network, using a pre-made good embedding, in this case a GloVe embedding I found in Kaggle (https://www.kaggle.com/takuok/glove840b300dtxt), as in this way, I can use it directly in a Kaggle kernel just by adding the data into the kernel. Regarding the embedding dimensions, I didn't find the model to change much when changing from, say, 50 to 70, mainly because of the few number of words I have in the questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\nIf you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\nIn case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "random.seed(1235)\n",
    "img_h = 320\n",
    "img_w = 480 \n",
    "max_words = 100\n",
    "embedding_dim = 70\n",
    "num_classes = 13\n",
    "val_split = 0.8\n",
    "max_len = 25\n",
    "    \n",
    "classes = [ '0',\n",
    "            '1',\n",
    "            '10',\n",
    "            '2',\n",
    "            '3',\n",
    "            '4',\n",
    "            '5',\n",
    "            '6',\n",
    "            '7',\n",
    "            '8',\n",
    "            '9',\n",
    "            'no',\n",
    "            'yes'\n",
    "          ]\n",
    "\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoder_ = label_encoder.fit(classes)\n",
    "integer_encoded = integer_encoder_.transform(classes)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoder_ = onehot_encoder.fit(integer_encoded)\n",
    "\n",
    "\n",
    "def data_generator(mode, tokenizer, batch_size = 12):\n",
    "\n",
    "    \n",
    "    with open('/kaggle/input/ann-and-dl-vqa/dataset_vqa/train_data.json', 'r') as f:\n",
    "          data_raw = json.load(f)\n",
    "    f.close()\n",
    "    while True:\n",
    "        # Select files (paths/indices) for the batch randomly\n",
    "\n",
    "        if mode == 'validation':\n",
    "            batch_addresses = random.sample(range(int(len(data_raw['questions'])*val_split),len(data_raw['questions'])), batch_size)\n",
    "        elif mode == 'train':\n",
    "            batch_addresses = random.sample(range(0, int(len(data_raw['questions'])*val_split)), batch_size)\n",
    "        else:\n",
    "             batch_addresses = random.sample(range(0, len(data_raw['questions'])), batch_size)\n",
    "            \n",
    "        batch_input_img = []\n",
    "        batch_input_txt = []\n",
    "        batch_output = [] \n",
    "\n",
    "        for i in batch_addresses:\n",
    "            #get the image in the required format\n",
    "            image_name = data_raw['questions'][i]['image_filename']\n",
    "            img = Image.open('/kaggle/input/ann-and-dl-vqa/dataset_vqa/train/' + image_name).convert('RGB')\n",
    "            img_array = np.array(img)\n",
    "            img_array = np.expand_dims(img_array, 0)\n",
    "            input_img = np.true_divide(img_array,255)\n",
    "\n",
    "            input_txt = data_raw['questions'][i]['question']\n",
    "\n",
    "            output = data_raw['questions'][i]['answer']\n",
    "\n",
    "            batch_input_img += [ input_img ]\n",
    "            batch_input_txt += [ input_txt ]\n",
    "            batch_output += [ output ]\n",
    "            \n",
    "            # Return a tuple of (input,output) to feed the network\n",
    "            batch_x_img = np.array( batch_input_img )\n",
    "            batch_x_txt = np.array( batch_input_txt )\n",
    "\n",
    "        batch_x_img = batch_x_img[:,-1]    \n",
    "        # prepare sequences with tokens and padding\n",
    "        tokenized = tokenizer.texts_to_sequences(batch_x_txt)\n",
    "        batch_x_txt = pad_sequences(tokenized, padding='post', maxlen=max_len) \n",
    "        \n",
    "    \n",
    "        batch_y = np.array( batch_output )\n",
    "        y_c = integer_encoder_.transform(batch_y)\n",
    "        y_c = y_c.reshape(len(y_c), 1)\n",
    "        onehot_encoded = onehot_encoder_.transform(y_c)\n",
    "\n",
    "        batch_y = onehot_encoded\n",
    "\n",
    "        #batch_x_txt = np.expand_dims(batch_x_txt, axis=-1) Se uso solo LSTM senza Embedding\n",
    "\n",
    "        yield ([batch_x_img,batch_x_txt], batch_y )\n",
    "        \n",
    "#same for the test generator apart from the fact that here we do not work with batches\n",
    "def test_generator():\n",
    "    \n",
    "    \n",
    "    with open('/kaggle/input/ann-and-dl-vqa/dataset_vqa/test_data.json', 'r') as f:\n",
    "          data_raw = json.load(f)\n",
    "    f.close()\n",
    "    i = 0\n",
    "    while (i<=len(data_raw['questions'])):\n",
    "\n",
    "        batch_input_img = []\n",
    "        batch_input_txt = []\n",
    "        batch_output = [] \n",
    "\n",
    "        image_name = data_raw['questions'][i]['image_filename']\n",
    "        img = Image.open('/kaggle/input/ann-and-dl-vqa/dataset_vqa/test/' + image_name).convert('RGB')\n",
    "        img_array = np.array(img)\n",
    "        img_array = np.expand_dims(img_array, 0)\n",
    "        input_img = np.true_divide(img_array,255)\n",
    "\n",
    "        input_txt = data_raw['questions'][i]['question']\n",
    "\n",
    "        output = data_raw['questions'][i]['question_id']\n",
    "\n",
    "        batch_input_img += [ input_img ]\n",
    "        batch_input_txt += [ input_txt ]\n",
    "\n",
    "        # Return a tuple of (input,output) to feed the network\n",
    "        batch_x_img = np.array( batch_input_img )\n",
    "        batch_x_txt = np.array( batch_input_txt )\n",
    "\n",
    "        batch_x_img = batch_x_img[:,-1]    \n",
    "\n",
    "        tokenized = tokenizer.texts_to_sequences(batch_x_txt)\n",
    "        batch_x_txt = pad_sequences(tokenized, padding='post', maxlen=max_len) \n",
    "        \n",
    "        batch_y = output\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "        #batch_x_txt = np.expand_dims(batch_x_txt, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "        yield ([batch_x_img,batch_x_txt], batch_y )\n",
    "           \n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    #I search in the embedding text file the words in order to build the embedding matrix\n",
    "    with open(filepath) as f:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index and count<(len(word_index)-1):\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "                count = count + 1\n",
    "\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def create_tokens(tokenizer,mode = 0):\n",
    "    # mode = 1 per avere la serie delle risposte, in modo da poterle analizzare (frequenza ecc)\n",
    "    \n",
    "    #i cycle through the training data in order to get all the needed words\n",
    "    with open('/kaggle/input/ann-and-dl-vqa/dataset_vqa/train_data.json', 'r') as f:\n",
    "          data_raw = json.load(f)\n",
    "    f.close()\n",
    "    tot_txt = []\n",
    "    container = []\n",
    "    for i in range(len(data_raw['questions'])):\n",
    "        input_txt = data_raw['questions'][i]['question']\n",
    "        tot_txt += [input_txt]\n",
    "        if mode:\n",
    "            container.append(data_raw['questions'][i]['answer'])  \n",
    "        \n",
    "    tokenizer.fit_on_texts(tot_txt)\n",
    "    if mode:\n",
    "        return (tokenizer,container)  \n",
    "    else:\n",
    "        return tokenizer \n",
    "\n",
    "def creator(max_words):\n",
    "    tokenizer = Tokenizer(num_words=max_words,oov_token = 'OOV')\n",
    "    tokenizer = create_tokens(tokenizer)\n",
    "    filepath = \"../input/glove840b300dtxt/\" + os.listdir(\"../input/glove840b300dtxt/\")[0]\n",
    "\n",
    "    embedding_matrix = create_embedding_matrix(filepath, tokenizer.word_index, embedding_dim)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    reader = data_generator('train',tokenizer)\n",
    "    \n",
    "    return tokenizer,embedding_matrix,vocab_size,reader\n",
    "\n",
    "(tokenizer,embedding_matrix,vocab_size,reader) = creator(max_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I looked at some indicators to see whether my embedding was good enough (as first I was using a lighter, much worse embedding), such as the cosine similarity between words I knew had similar or really different meaning.\nI also saw that the dataset is really unbalanced. It might help to create a personalised loss in order to weigh more some results. \n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#DIAGNOSTICS\n",
    "\"\"\"\n",
    "print(tokenizer.word_index)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_sim = cosine_similarity(embedding_matrix)\n",
    "i=31\n",
    "c = cos_sim[i]\n",
    "np.argsort(c)\n",
    "print(c[29])\n",
    "\"\"\"\n",
    "# l'embedding sembra buono dato che le parole simili hanno una cosine similarity più alta\n",
    "\n",
    "#plt.hist(answers, density= True) #il dataset è molto sbilanciato (= yes or no, che rende bilanciata la classificazione, ma 85% di counting è <= 3 )\n",
    "# 30 % delle domande sono di classificazione, un po' meno, anche se non tragico\n",
    "#Questo che conseguenze ha sulle performance di un weak learner?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first naive model I used. It basically consists in a image-embedding branch, which uses features extracted by VGG16 after a globablmaxpooling and question-embedding branch, which consists of a word embedding layer (with the obtained embedding matrix) and two bidirectional LSTM layers.\nThese two feature vectors are then concatenated and, after a dense layer, we have the classification dense layer, which outputs the result. This model works okay and gets to ~ 50% accuracy, without spending too much time tuning the hyperparameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_net(epochs,batch_size):\n",
    "\n",
    "    arch =  tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_shape=(img_h, img_w, 3))\n",
    "\n",
    "    freeze_until = 800\n",
    "    for layer in arch.layers[:freeze_until]:\n",
    "          layer.trainable = False\n",
    "    branch1 = arch.output\n",
    "\n",
    "    #branch1 = tf.keras.layers.Flatten() (branch1)\n",
    "    branch1 = tf.keras.layers.GlobalMaxPooling2D() (branch1)\n",
    "    #branch1 = tf.keras.layers.Dense(256, activation='tanh') (branch1)\n",
    "\n",
    "\n",
    "\n",
    "    text_inputs = tf.keras.Input(shape=[max_len])\n",
    "\n",
    "    \n",
    "    #bidirectional to catch all the context, low dropout as it worsens the performance (like this the net already has too little power)\n",
    "    emb = tf.keras.layers.Embedding(vocab_size,embedding_dim, \n",
    "                               input_length=max_words, \n",
    "                               weights=[embedding_matrix], \n",
    "                               trainable=False)  (text_inputs)\n",
    "\n",
    "    branch2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences=True))(emb)\n",
    "    branch2 = tf.keras.layers.Activation('tanh')(branch2)\n",
    "    branch2 = tf.keras.layers.Dropout(0.2)(branch2)\n",
    "    branch2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(branch2)\n",
    "    branch2 = tf.keras.layers.Activation('tanh')(branch2)\n",
    "    branch2 = tf.keras.layers.Dropout(0.2)(branch2)\n",
    "\n",
    "\n",
    "    #straightforward concatenation\n",
    "    joint = tf.keras.layers.concatenate([branch1, branch2])\n",
    "    joint = tf.keras.layers.Dense(1024, activation='relu')(joint)\n",
    "    joint = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(joint)\n",
    "    joint = tf.keras.layers.Activation('relu')(joint)\n",
    "    joint = tf.keras.layers.Dropout(0.1)(joint)\n",
    "\n",
    "    predictions = tf.keras.layers.Dense(num_classes, activation='softmax')(joint)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[arch.input, text_inputs], outputs=[predictions])\n",
    "\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    lr = 5e-4\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    model.compile(loss = loss,\n",
    "                       optimizer = optimizer,\n",
    "                       metrics = ['accuracy'])\n",
    "\n",
    "    callbacks=[]\n",
    "    callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10,restore_best_weights=True))\n",
    "\n",
    "\n",
    "    callbacks.append(tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "\n",
    "    try:\n",
    "        history = model.fit_generator(data_generator('train',tokenizer),validation_data = data_generator('validation',tokenizer), steps_per_epoch= 200, validation_steps = 100, epochs=epochs, callbacks=callbacks,  verbose=1, workers=8, use_multiprocessing=True, max_queue_size=100)\n",
    "        model.save('model.h5')\n",
    "    except KeyboardInterrupt:\n",
    "        model.save('model.h5')\n",
    "\n",
    "    return (history,model)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 12\n",
    "(history,model) = train_net(epochs,batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then tried to better my model. I had two main ideas, the first being to implement attention and the second to develop a on/off switch based on the question type (existence vs counting) in order to train two different neural networks for the different tasks, using a regression loss for counting and a classification loss for existence. While this might work, I have no evidence and, in the end, as I wasn't sure, I decided not to try it, even because it would have meant simplifying the networks, as Kaggle kernels have a low constraint on RAM. For the future, I could still design a personalised fuzzy loss, in order to let the network see as a major error the misclassification of the question and have an error rate proportional to the difference from the correct count in counting, keeping in mind I have to let the loss be differentiable in order for backprop to work. Again, I don't know if it would work.\nI, however, tried with attention, but even here I found to have a constraint on RAM. In fact, I thought of using a top-down attention module (similar to this one https://arxiv.org/pdf/1708.02711.pdf), but the k * n final features would probably be too much. In the end, I adapted https://github.com/adamcasson/show_ask_attend_answer, an implementation of the famous show-ask-attend paper in keras (I actually found the majority of vqa implemented in Pytorch (and some in pure TF)), which didn't really get great results, though. It is also to be noted that it's a poor attention mechanism and could be bettered in many ways.\nLike this, without too much hp tuning, I got to ~ 53%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_3 (InputLayer)            [(None, 320, 480, 3) 0                                            \n__________________________________________________________________________________________________\nblock1_conv1 (Conv2D)           (None, 320, 480, 64) 1792        input_3[0][0]                    \n__________________________________________________________________________________________________\nblock1_conv2 (Conv2D)           (None, 320, 480, 64) 36928       block1_conv1[0][0]               \n__________________________________________________________________________________________________\nblock1_pool (MaxPooling2D)      (None, 160, 240, 64) 0           block1_conv2[0][0]               \n__________________________________________________________________________________________________\nblock2_conv1 (Conv2D)           (None, 160, 240, 128 73856       block1_pool[0][0]                \n__________________________________________________________________________________________________\nblock2_conv2 (Conv2D)           (None, 160, 240, 128 147584      block2_conv1[0][0]               \n__________________________________________________________________________________________________\nblock2_pool (MaxPooling2D)      (None, 80, 120, 128) 0           block2_conv2[0][0]               \n__________________________________________________________________________________________________\nblock3_conv1 (Conv2D)           (None, 80, 120, 256) 295168      block2_pool[0][0]                \n__________________________________________________________________________________________________\nblock3_conv2 (Conv2D)           (None, 80, 120, 256) 590080      block3_conv1[0][0]               \n__________________________________________________________________________________________________\nblock3_conv3 (Conv2D)           (None, 80, 120, 256) 590080      block3_conv2[0][0]               \n__________________________________________________________________________________________________\ninput_4 (InputLayer)            [(None, 25)]         0                                            \n__________________________________________________________________________________________________\nblock3_pool (MaxPooling2D)      (None, 40, 60, 256)  0           block3_conv3[0][0]               \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 25, 70)       5040        input_4[0][0]                    \n__________________________________________________________________________________________________\nblock4_conv1 (Conv2D)           (None, 40, 60, 512)  1180160     block3_pool[0][0]                \n__________________________________________________________________________________________________\nbidirectional_2 (Bidirectional) (None, 25, 312)      283296      embedding_1[0][0]                \n__________________________________________________________________________________________________\nblock4_conv2 (Conv2D)           (None, 40, 60, 512)  2359808     block4_conv1[0][0]               \n__________________________________________________________________________________________________\nactivation_5 (Activation)       (None, 25, 312)      0           bidirectional_2[0][0]            \n__________________________________________________________________________________________________\nblock4_conv3 (Conv2D)           (None, 40, 60, 512)  2359808     block4_conv2[0][0]               \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 25, 312)      0           activation_5[0][0]               \n__________________________________________________________________________________________________\nblock4_pool (MaxPooling2D)      (None, 20, 30, 512)  0           block4_conv3[0][0]               \n__________________________________________________________________________________________________\nbidirectional_3 (Bidirectional) (None, 256)          451584      dropout_3[0][0]                  \n__________________________________________________________________________________________________\nblock5_conv1 (Conv2D)           (None, 20, 30, 512)  2359808     block4_pool[0][0]                \n__________________________________________________________________________________________________\nactivation_6 (Activation)       (None, 256)          0           bidirectional_3[0][0]            \n__________________________________________________________________________________________________\nblock5_conv2 (Conv2D)           (None, 20, 30, 512)  2359808     block5_conv1[0][0]               \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 256)          0           activation_6[0][0]               \n__________________________________________________________________________________________________\nblock5_conv3 (Conv2D)           (None, 20, 30, 512)  2359808     block5_conv2[0][0]               \n__________________________________________________________________________________________________\nrepeat_vector_2 (RepeatVector)  (None, 150, 256)     0           dropout_4[0][0]                  \n__________________________________________________________________________________________________\nblock5_pool (MaxPooling2D)      (None, 10, 15, 512)  0           block5_conv3[0][0]               \n__________________________________________________________________________________________________\nreshape_3 (Reshape)             (None, 10, 15, 256)  0           repeat_vector_2[0][0]            \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 10, 15, 768)  0           block5_pool[0][0]                \n                                                                 reshape_3[0][0]                  \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 10, 15, 512)  393728      concatenate_2[0][0]              \n__________________________________________________________________________________________________\nactivation_7 (Activation)       (None, 10, 15, 512)  0           conv2d_2[0][0]                   \n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 10, 15, 512)  0           activation_7[0][0]               \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 10, 15, 2)    1026        dropout_5[0][0]                  \n__________________________________________________________________________________________________\nactivation_8 (Activation)       (None, 10, 15, 2)    0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nlambda_4 (Lambda)               (None, 10, 15)       0           activation_8[0][0]               \n__________________________________________________________________________________________________\nlambda_6 (Lambda)               (None, 10, 15)       0           activation_8[0][0]               \n__________________________________________________________________________________________________\nreshape_4 (Reshape)             (None, 10, 15, 1)    0           lambda_4[0][0]                   \n__________________________________________________________________________________________________\nreshape_5 (Reshape)             (None, 10, 15, 1)    0           lambda_6[0][0]                   \n__________________________________________________________________________________________________\nlambda_5 (Lambda)               (None, 10, 15, 512)  0           reshape_4[0][0]                  \n__________________________________________________________________________________________________\nlambda_7 (Lambda)               (None, 10, 15, 512)  0           reshape_5[0][0]                  \n__________________________________________________________________________________________________\nmultiply_3 (Multiply)           (None, 10, 15, 512)  0           block5_pool[0][0]                \n                                                                 lambda_5[0][0]                   \n__________________________________________________________________________________________________\nmultiply_4 (Multiply)           (None, 10, 15, 512)  0           block5_pool[0][0]                \n                                                                 lambda_7[0][0]                   \n__________________________________________________________________________________________________\naverage_pooling2d_2 (AveragePoo (None, 1, 1, 512)    0           multiply_3[0][0]                 \n__________________________________________________________________________________________________\naverage_pooling2d_3 (AveragePoo (None, 1, 1, 512)    0           multiply_4[0][0]                 \n__________________________________________________________________________________________________\nflatten_3 (Flatten)             (None, 512)          0           average_pooling2d_2[0][0]        \n__________________________________________________________________________________________________\nflatten_4 (Flatten)             (None, 512)          0           average_pooling2d_3[0][0]        \n__________________________________________________________________________________________________\nrepeat_vector_3 (RepeatVector)  (None, 4, 256)       0           dropout_4[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 1024)         0           flatten_3[0][0]                  \n                                                                 flatten_4[0][0]                  \n__________________________________________________________________________________________________\nflatten_5 (Flatten)             (None, 1024)         0           repeat_vector_3[0][0]            \n__________________________________________________________________________________________________\nmultiply_5 (Multiply)           (None, 1024)         0           concatenate_3[0][0]              \n                                                                 flatten_5[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 1024)         4096        multiply_5[0][0]                 \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1024)         1049600     batch_normalization_2[0][0]      \n__________________________________________________________________________________________________\nbatch_normalization_3 (BatchNor (None, 1024)         4096        dense_2[0][0]                    \n__________________________________________________________________________________________________\nactivation_9 (Activation)       (None, 1024)         0           batch_normalization_3[0][0]      \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 13)           13325       activation_9[0][0]               \n==================================================================================================\nTotal params: 16,920,479\nTrainable params: 2,196,655\nNon-trainable params: 14,723,824\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def glimpse(attention_maps, image_features, num_glimpses=2, n1=10, n2=15):\n",
    "    glimpse_list = []\n",
    "    for i in range(num_glimpses):\n",
    "        glimpse_map = tf.keras.layers.Lambda(lambda x: x[:,:,:,i])(attention_maps)                # Select the i'th attention map\n",
    "        glimpse_map = tf.keras.layers.Reshape((n1,n2,1))(glimpse_map)                             # Reshape to add channel dimension for K.tile() to work. (14,14) --> (14,14,1)\n",
    "        glimpse_tile = tf.keras.layers.Lambda(tile)(glimpse_map)                                  # Repeat the attention over the channel dimension. (14,14,1) --> (14,14,2048)\n",
    "        weighted_features = tf.keras.layers.multiply([image_features, glimpse_tile])              # Element wise multiplication to weight image features\n",
    "        weighted_average = tf.keras.layers.AveragePooling2D(pool_size=(n1,n2))(weighted_features) # Average pool each channel. (14,14,512) --> (1,1,512)\n",
    "        weighted_average = tf.keras.layers.Flatten()(weighted_average)\n",
    "        glimpse_list.append(weighted_average)\n",
    "        \n",
    "    return tf.keras.layers.concatenate(glimpse_list)\n",
    "\n",
    "def tile(x):\n",
    "    return tf.keras.backend.tile(x, [1,1,1,n_f])\n",
    "\n",
    "def train_net(epochs,batch_size,n1,n2,dropout):\n",
    "\n",
    "\n",
    "\n",
    "    #Troppo grosso layer finale per inceptionresnet\n",
    "    #arch =  tf.keras.applications.densenet.DenseNet201(include_top=False, weights='imagenet', input_shape=(img_h, img_w, 3)) #1920\n",
    "    #arch = tf.keras.applications.resnet_v2.ResNet152V2(include_top=False, weights='imagenet',input_shape=(img_h, img_w, 3)) #2048\n",
    "    arch =  tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_shape=(img_h, img_w, 3)) #512\n",
    "\n",
    "    for layer in arch.layers[:800]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    branch1 = arch.output\n",
    "\n",
    "\n",
    "\n",
    "    text_inputs = tf.keras.Input(shape=[max_len])\n",
    "\n",
    "    #simple embedding\n",
    "    emb = tf.keras.layers.Embedding(vocab_size,embedding_dim, \n",
    "                               input_length=max_words, \n",
    "                               weights=[embedding_matrix], \n",
    "                               trainable=False)  (text_inputs)\n",
    "\n",
    "    branch2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(156,return_sequences=True))(emb)\n",
    "    branch2 = tf.keras.layers.Activation('tanh')(branch2)\n",
    "    branch2 = tf.keras.layers.Dropout(dropout)(branch2)\n",
    "    branch2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(branch2)\n",
    "    branch2 = tf.keras.layers.Activation('tanh')(branch2)\n",
    "    branch2 = tf.keras.layers.Dropout(dropout)(branch2)\n",
    "    \n",
    "    #i need to reshape it in order to use keras tensor operations\n",
    "    question_tile = tf.keras.layers.RepeatVector(n1*n2)(branch2)\n",
    "    question_tile = tf.keras.layers.Reshape((n1,n2,int(n_f/2)))(question_tile)\n",
    "\n",
    "    joint = tf.keras.layers.concatenate([branch1, question_tile])\n",
    "    \n",
    "    #here i'm actually building the attention maps by using the image and the text\n",
    "    attention = tf.keras.layers.Conv2D(n_f,(1,1))(joint)\n",
    "    attention_relu = tf.keras.layers.Activation('relu')(attention)\n",
    "    attention_relu = tf.keras.layers.Dropout(dropout)(attention_relu)\n",
    "    \n",
    "    attention_conv = tf.keras.layers.Conv2D(n_gli, (1,1))(attention_relu)\n",
    "    attention_maps = tf.keras.layers.Activation('softmax')(attention_conv)\n",
    "    \n",
    "    image_attention = glimpse(attention_maps, branch1, n_gli, n1,n2)\n",
    "    \n",
    "    #here I've tried a different approach in the end, by multiplying the text features for attention. While it makes sense in general,\n",
    "    #here I've had to take into account the different dimensions and therefore to multiply the attention maps for a concatenation of\n",
    "    #4 text feature vectors so I'm not really sure it makes sense, but it is the one which works best (commented you have the previous version )\n",
    "    \n",
    "    branch2_n = tf.keras.layers.RepeatVector(n_gli*2)(branch2)\n",
    "    branch2_n = tf.keras.layers.Flatten() (branch2_n)\n",
    "    joint2 = tf.keras.layers.multiply([image_attention,branch2_n])\n",
    "    \n",
    "    #joint2 = tf.keras.layers.concatenate([image_attention,branch2])\n",
    "    #joint2 = tf.keras.layers.Dropout(dropout)(joint2)\n",
    "    \n",
    "\n",
    "    joint2 = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(joint2)\n",
    "    \n",
    "    #dense layer in the end \n",
    "    joint_fc = tf.keras.layers.Dense(1024)(joint2)\n",
    "    joint_fc = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(joint_fc)\n",
    "    joint_fc = tf.keras.layers.Activation('relu')(joint_fc)\n",
    "\n",
    "    predictions = tf.keras.layers.Dense(num_classes, activation='softmax')(joint_fc)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[arch.input, text_inputs], outputs=[predictions])\n",
    "\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    lr = 5e-4\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    model.compile(loss = loss,\n",
    "                       optimizer = optimizer,\n",
    "                       metrics = ['accuracy'])\n",
    "    \n",
    "    #as i soon go OOM, I can easily restart training if needed, having saved the weights with a callback\n",
    "    filepath = \"../input/weightsnn/\" + os.listdir(\"../input/weightsnn/\")[0]\n",
    "    model.load_weights(filepath)\n",
    "\n",
    "    callbacks=[]\n",
    "    callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10,restore_best_weights=True))\n",
    "\n",
    "    callbacks.append(tf.keras.callbacks.ModelCheckpoint('weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1))\n",
    "\n",
    "    callbacks.append(tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "    \n",
    "    #return (model,2) (if I just have to return the model)\n",
    "    try:\n",
    "        history = model.fit_generator(data_generator('train',tokenizer,batch_size),validation_data = data_generator('validation',tokenizer,batch_size), steps_per_epoch = 200, validation_steps = 80, epochs=epochs, callbacks=callbacks,  verbose=1, workers=8, use_multiprocessing=True, max_queue_size=100)\n",
    "        model.save('model.h5')\n",
    "        return (model,history)\n",
    "    except KeyboardInterrupt:\n",
    "        model.save('model.h5')\n",
    "        return (model,2)\n",
    "    \n",
    "\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "n1 = 10\n",
    "n2 = 15\n",
    "n_f = 512 # it has to be the number of feature maps for last layer of the cnn network\n",
    "n_gli = 2 # number of glimpses (more glimpses = more time + more imbalance in last layer + possible ram error)\n",
    "dropout = 0.1\n",
    "(model,history) = train_net(epochs,batch_size,n1,n2,dropout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model)\n",
    "#filepath = \"../input/weightsnn/\" + os.listdir(\"../input/weightsnn/\")[0]\n",
    "#model.load_weights(filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I get the predictions using my test generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_gen = test_generator()\n",
    "results = {}\n",
    "count = 0 \n",
    "\n",
    "while 1:\n",
    "    inputs, outputs = next(test_gen)\n",
    "    pred = model.predict(inputs)\n",
    "    results[outputs]=np.argmax(pred)\n",
    "    print(count)\n",
    "    count = count +1 \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet was useful in order to perform diagnostics for the model, mainly in the early stages, when a fault in the training generator, made the training useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#DIAGNOSTICS\n",
    "\n",
    "with open('/kaggle/input/ann-and-dl-vqa/dataset_vqa/test_data.json', 'r') as f:\n",
    "          test_data = json.load(f)\n",
    "f.close()\n",
    "\n",
    "test_gen = test_generator()\n",
    "train_gen = data_generator('train',tokenizer)\n",
    "count = 0\n",
    "\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "\n",
    "while count<10:\n",
    "    inputs, outputs = next(train_gen,tokenizer)\n",
    "    #print(test_data['questions'][count]['question'])\n",
    "    #print(inputs[1])\n",
    "    print(list(map(sequence_to_text, inputs[1])))\n",
    "    print(outputs)\n",
    "\n",
    "\"\"\"\n",
    "while count<5:\n",
    "    inputs, outputs = next(test_gen)\n",
    "    pred = model.predict(inputs)\n",
    "    print(test_data['questions'][count]['question'])\n",
    "    #print(inputs[1])\n",
    "    print(list(map(sequence_to_text, inputs[1])))\n",
    "    count = count +1 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model, to_file='model.png')\n",
    "\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def create_csv(results, results_dir='./'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(str(key) + ',' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "create_csv(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#plt.hist(results.values(), density= True)\n",
    "results.values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
